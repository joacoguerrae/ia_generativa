{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico: Modelos de Difusión para la Generación de Imágenes\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Los modelos de difusión son una clase de modelos generativos que aprenden a generar datos de alta calidad revirtiendo un proceso de ruido progresivo. A medida que el modelo se entrena, aprende a deshacer el ruido para recuperar las imágenes originales. En este trabajo práctico, utilizaremos un modelo de difusión preentrenado para generar imágenes del conjunto de datos **CIFAR-10**, un dataset comúnmente usado en visión por computadora que contiene imágenes de 32x32 píxeles de 10 clases diferentes (como perros, gatos, automóviles, etc.).\n",
    "\n",
    "A lo largo de este notebook, exploraremos cómo el modelo genera imágenes a partir de ruido y cómo diferentes parámetros afectan este proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Instalamos las librerías necesarias\n",
    "!pip install transformers diffusers imageio -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Preparación del entorno\n",
    "\n",
    "Primero, instalaremos y cargaremos las librerías necesarias. Utilizaremos **PyTorch** y el modelo preentrenado desde **Hugging Face**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import time\n",
    "import imageio\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from diffusers import DDPMPipeline\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Crear las carpetas './imgs' y './grid' si no existen\n",
    "os.makedirs(\"./imgs\", exist_ok=True)\n",
    "os.makedirs(\"./grid\", exist_ok=True)\n",
    "os.makedirs(\"./gifs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Cargar el modelo preentrenado\n",
    "\n",
    "Vamos a utilizar un modelo de difusión preentrenado en **CIFAR-10**. Hugging Face ofrece un pipeline ya entrenado que facilita la generación de imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo pre-entrenado a utilizar se llama DDPM (Denoising Diffusion Probabilistic Models) de Jonathan Ho, Ajay Jain and Pieter Abbeel. Se puede encontrar la documentación en el siguiente link https://huggingface.co/docs/diffusers/api/pipelines/ddpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo preentrenado DDPMPipeline para CIFAR-10\n",
    "model_id = \"google/ddpm-cifar10-32\"\n",
    "pipeline = DDPMPipeline.from_pretrained(model_id)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Generación de imágenes a partir de ruido\n",
    "\n",
    "En esta sección, generaremos imágenes usando el modelo preentrenado. Cada imagen comenzará como ruido y el modelo lo \"des-noiseará\" gradualmente para producir una imagen reconocible.\n",
    "\n",
    "El proceso de generación toma un número de **timesteps**, que son los pasos intermedios durante los cuales el modelo elimina progresivamente el ruido para obtener una imagen. Podemos ajustar el número de timesteps para ver cómo afecta a la calidad de la imagen.\n",
    "\n",
    "### Ejercicio:\n",
    "- ¿Qué sucede durante los “timesteps” en el proceso de generación de imágenes utilizando modelos de difusión?\n",
    "- ¿Por qué comenzamos con una imagen completamente de ruido? ¿Qué papel juega el ruido en el proceso de generación de imágenes?\n",
    "- Si la imagen original está completamente cubierta de ruido, ¿cómo es posible que el modelo aprenda a des-noisearla hasta obtener una imagen clara?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imgs(imgs):\n",
    "    # Mostrar las imágenes generadas\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, img in enumerate(imgs[:16]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar imágenes\n",
    "imgs = pipeline(num_inference_steps=1).images\n",
    "plot_imgs(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Explorando los timesteps.\n",
    "\n",
    "El número de pasos de inferencia (`num_inference_steps`) refiere a los timesteps mencionados anteriormente.\n",
    "\n",
    "### Ejercicio:\n",
    "- Completar código.\n",
    "- Modificar la cantidad de timesteps `num_inference_steps` para ver como varía el resultado (y e tiempo de inferencia) y analizar su impacto.\n",
    "- ¿Cómo afecta el número de timesteps (pasos de inferencia) a la calidad de las imágenes generadas?\n",
    "- ¿Cómo describirías visualmente las diferencias entre las imágenes generadas con 10 timesteps y 50 timesteps? ¿Qué esperas observar en cuanto a detalle y nitidez?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generar 16 imágenes con distintos pasos de inferencia y plotearlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: Manipulación del ruido\n",
    "\n",
    "El ruido es una parte crítica del proceso de difusión. A partir de un ruido aleatorio, el modelo de difusión aprende a generar una imagen. Sin embargo, huggingface nos permite manipular la generación de este ruido pasandole una `seed`.\n",
    "\n",
    "### Ejercicio\n",
    "- Completar código.\n",
    "- ¿Qué es una semilla (seed) y por qué es útil en el proceso de generación de ruido en modelos de difusión?\n",
    "- ¿Cómo cambia la imagen generada si utilizas diferentes semillas para el ruido inicial? ¿Esperas que dos imágenes generadas con diferentes semillas sean completamente diferentes o tengan similitudes?\n",
    "- Si utilizas la misma semilla para generar ruido dos veces, ¿esperas obtener la misma imagen al final del proceso de denoising? ¿Por qué?\n",
    "- Modificar la semilla de generación de ruido para ver cómo varía el resultado y ver cómo impacta en la imagen generada.\n",
    "- ¿Qué observas cuando cambias la semilla pero mantienes el resto de los parámetros del modelo constante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experimenta cambiando la semilla del generador de ruido (parámetro `generator` de la función `pipeline`)\n",
    "generator = torch.manual_seed(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6: Generación de imágenes en batch y visualización en grilla\n",
    "\n",
    "Este proceso genera múltiples imágenes a la vez utilizando un **batch size** definido, y luego organiza esas imágenes en una grilla para visualizarlas.\n",
    "\n",
    "### Proceso:\n",
    "1. **Batch de ruido**: Se genera un batch de imágenes de ruido gaussiano utilizando un tamaño de batch específico (por ejemplo, `batch_size=8`).\n",
    "2. **Generación en paralelo**: El pipeline genera varias imágenes en paralelo utilizando el batch de ruido.\n",
    "3. **Grilla de imágenes**: Las imágenes generadas se organizan en una grilla utilizando la función `torchvision.utils.make_grid`, lo que permite visualizar varias imágenes en una única figura.\n",
    "\n",
    "### Ejercicio:\n",
    "- Completar código.\n",
    "- Compare el tiempo de inferencia cuando genera en paralelo y secuencialmente (una por una).\n",
    "- Compare el tiempo de inferencia cuando genera varias imagenes en paralelo y una sola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = ...\n",
    "\n",
    "# Convertir las imágenes a tensores\n",
    "image_tensors = [torch.tensor(np.array(img)).permute(2, 0, 1).unsqueeze(0) / 255.0 for img in generated_images]\n",
    "image_tensors = torch.cat(image_tensors)\n",
    "\n",
    "# Crear una grilla con las imágenes generadas\n",
    "grid = make_grid(image_tensors, nrow=4)\n",
    "\n",
    "# Guardar la grilla como una imagen PNG\n",
    "save_image(grid, \"grid/generated_images_grid.png\")\n",
    "\n",
    "# Visualizar la grilla de imágenes\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 7: Generación paso a paso con el scheduler\n",
    "\n",
    "En este proceso, utilizamos el **scheduler** del pipeline para gestionar la eliminación de ruido de una imagen inicial ruidosa, paso a paso.\n",
    "\n",
    "### Proceso:\n",
    "\n",
    "1. **Imagen inicial**: Comenzamos con una imagen de ruido gaussiano.\n",
    "2. **Scheduler**: El **scheduler** controla cómo se elimina el ruido en cada paso, utilizando el modelo **UNet** para predecir y reducir el ruido progresivamente. El scheduler se encarga de realizar los pasos de denoising de manera estable.\n",
    "3. **Captura de imágenes intermedias**: Guardamos las imágenes intermedias en intervalos específicos para visualizar cómo la imagen final emerge del ruido.\n",
    "4. **Visualización**: Al final, se crea un GIF mostrando todo el proceso de generación, desde el ruido hasta la imagen final.\n",
    "\n",
    "### Ejercicio:\n",
    "- Completar código.\n",
    "- ¿Qué observas al capturar imágenes intermedias durante el proceso de denoising? ¿Cómo cambia la imagen en cada paso?\n",
    "- Explore con los parámetros `step_size` y `num_inference_steps`. ¿Para qué sirven?\n",
    "- ¿Qué podemos aprender del GIF generado a partir de imágenes intermedias? ¿Qué observas sobre el proceso desde una imagen completamente ruidosa hasta una clara?\n",
    "- ¿Por qué es útil visualizar el proceso de denoising paso a paso? ¿Cómo te ayuda esto a entender el funcionamiento del modelo de difusión?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar imágenes paso a paso con acceso al ruido intermedio\n",
    "def generate_images_with_intermediate_steps(pipeline, noise, scheduler, num_inference_steps, step_size=1):\n",
    "    images = []\n",
    "    image = noise  # Imagen inicial con ruido\n",
    "\n",
    "    for t in range(num_inference_steps):\n",
    "        # Obtener el valor de timestep actual\n",
    "        current_timestep = scheduler.timesteps[t]\n",
    "\n",
    "        # El modelo predice el ruido en la imagen\n",
    "        with torch.no_grad():\n",
    "            model_output = pipeline.unet(image, current_timestep).sample  # Predecir el ruido\n",
    "\n",
    "        # Realizar el paso del scheduler para actualizar la imagen con menos ruido\n",
    "        image = scheduler.step(model_output, current_timestep, image).prev_sample\n",
    "\n",
    "        # Guardar las imágenes en intervalos de step_size\n",
    "        if t % step_size == 0:\n",
    "            images.append(image.clone())  # Guardar imagen intermedia\n",
    "            save_image(image, f\"imgs/step_{t}.png\")  # Guardar como PNG\n",
    "\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = ...\n",
    "num_inference_steps = ...\n",
    "img_shape = (1, 3, 32, 32)\n",
    "\n",
    "# Acceder al scheduler desde el pipeline\n",
    "scheduler = pipeline.scheduler\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "# Crear una imagen de ruido inicial (completamente ruidosa)\n",
    "initial_noise = ...\n",
    "\n",
    "# Generar las imágenes paso a paso\n",
    "images = generate_images_with_intermediate_steps(pipeline, initial_noise, scheduler, num_inference_steps, step_size)\n",
    "\n",
    "# Visualizar la última imagen generada\n",
    "final_image = images[-1].squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "plt.imshow(final_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "image_paths = [f\"imgs/step_{t}.png\" for t in range(0, num_inference_steps, step_size)]\n",
    "images_gif = [imageio.imread(path) for path in image_paths]\n",
    "imageio.mimsave(\"gifs/step_by_step.gif\", images_gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 8: Exploración del espacio latente entre dos imagenes\n",
    "\n",
    "Vamos explorar el espacio latente entre dos imágenes. El objetivo es ver si podemos encontrar alguna relación entre las imagenes generadas por el movimiento del espacio latente entre dos imagenes.\n",
    "\n",
    "### Proceso:\n",
    "1. Generar dos estructuras con ruido diferente.\n",
    "2. Interpolar entre ambas estructuras utilizando un valor de `alpha` que cambia de 0 a 1.\n",
    "3. Pasar este ruido por el modelo de difusión.\n",
    "4. Visualizar las imágenes generadas para observar la transición.\n",
    "\n",
    "### Ejercicio:\n",
    "- Completar código.\n",
    "- ¿Qué es el espacio latente en un modelo de difusión y cómo se relaciona con las imágenes generadas?\n",
    "- ¿Qué sucede cuando interpolamos entre dos estructuras de ruido diferentes utilizando un valor de alpha? ¿Qué esperarías ver en las imágenes generadas?\n",
    "- Al visualizar las imágenes generadas por la interpolación, ¿observas alguna relación coherente entre las imágenes? ¿Las imágenes de la transición muestran características de ambas imágenes originales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Completa la función `denoise_image` para realizar el denoising paso a paso. Hint: Basarse en la función `generate_images_with_intermediate_steps`\n",
    "# Función para realizar el denoising paso a paso\n",
    "# Dado un modelo UNet, una imagen ruidosa, un scheduler, el número de pasos de inferencia y el número de ruido que tenemos en las imágenes\n",
    "#   esta función elimina el ruido de la imagen paso a paso.\n",
    "def denoise_image(unet_model, noisy_image, scheduler, num_inference_steps, num_steps_added_noise=0):\n",
    "    image = ...\n",
    "    current_timestep = ...\n",
    "    for t in range(num_inference_steps - num_inference_steps, num_inference_steps):\n",
    "        # Predecir el ruido en la imagen actual\n",
    "\n",
    "        # Actualizar la imagen eliminando el ruido predicho\n",
    "        ...\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inference_steps = ...\n",
    "scheduler = ...\n",
    "# TODO: setear los timesteps del scheduler\n",
    "\n",
    "# Crear dos puntos del espacio latente (imágenes con ruido)\n",
    "image1_tensor = ...\n",
    "image2_tensor = ...\n",
    "\n",
    "# Interpolación entre las dos imágenes\n",
    "steps = 10\n",
    "interpolated_images = []\n",
    "for alpha in np.linspace(0, 1, steps):\n",
    "    # Interpolación de ruido entre las dos imágenes\n",
    "    interpolated_noise = (1 - alpha) * image1_tensor + alpha * image2_tensor\n",
    "\n",
    "    # Utilizamos el UNet del pipeline para predecir la imagen desde el ruido interpolado\n",
    "    interpolated_image = denoise_image(pipeline.unet, interpolated_noise, scheduler, num_inference_steps, num_inference_steps)\n",
    "\n",
    "    # Guardamos la imagen generada\n",
    "    interpolated_images.append(interpolated_image.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de las imágenes interpoladas\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i, img_tensor in enumerate(interpolated_images):\n",
    "    img = img_tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    plt.subplot(1, steps, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Crear el GIF de la interpolación\n",
    "images_for_gif = []\n",
    "for img_tensor in interpolated_images:\n",
    "    img = img_tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    img_uint8 = (img * 255).astype(np.uint8)  # Convertir a formato uint8 para el GIF\n",
    "    images_for_gif.append(img_uint8)\n",
    "\n",
    "# Guardar el GIF\n",
    "imageio.mimsave('gifs/interpolation.gif', images_for_gif, duration=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 9: Añadir ruido controlado a una imagen real y eliminarlo\n",
    "\n",
    "Vamos a tomar una imagen real del dataset, añadirle una cantidad controlada de ruido y luego intentaremos eliminar ese ruido utilizando el pipeline de difusión. Este proceso nos permitirá observar cómo el modelo reconstruye la imagen eliminando el ruido añadido.\n",
    "\n",
    "### Pasos:\n",
    "1. Seleccionar una imagen real del dataset CIFAR-10.\n",
    "2. Añadir ruido gaussiano controlado a la imagen.\n",
    "3. Usar el pipeline de difusión para eliminar el ruido y reconstruir la imagen original.\n",
    "\n",
    "Este ejercicio ayuda a entender cómo los modelos de difusión pueden realizar la tarea de denoising en imágenes.\n",
    "\n",
    "### Ejercicio:\n",
    "- Completar código.\n",
    "- ¿Qué es el ruido gaussiano y por qué lo añadimos a una imagen real en este ejercicio?\n",
    "- ¿Cómo afecta la cantidad de ruido añadido a la imagen al proceso de denoising? ¿Qué esperarías que ocurriera si añades mucho ruido en comparación con poco?\n",
    "- Si la cantidad de ruido añadido es muy alta, ¿crees que el modelo podrá eliminar todo el ruido y restaurar completamente la imagen original? ¿Por qué?\n",
    "- ¿En qué escenarios prácticos crees que se podría aplicar este proceso de denoising en imágenes reales? ¿Qué aplicaciones puedes imaginar para esta técnica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset CIFAR-10 y seleccionar una imagen\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "image, _ = next(iter(dataloader))  # Tomar una imagen del dataset\n",
    "\n",
    "# Acceder al UNet y el scheduler del pipeline\n",
    "unet_model = pipeline.unet\n",
    "scheduler = pipeline.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el scheduler con el número de pasos\n",
    "num_inference_steps = ...\n",
    "num_steps_added_noise = ...\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "# Añadir ruido en el último paso del scheduler\n",
    "timestep = scheduler.timesteps[-num_steps_added_noise]  # Tomar el último paso de la programación de ruido\n",
    "with torch.no_grad():\n",
    "    noisy_image = scheduler.add_noise(image, torch.randn_like(image), timestep)  # Añadir ruido\n",
    "\n",
    "# Aplicar el proceso de denoising a la imagen ruidosa\n",
    "denoised_image = ...\n",
    "\n",
    "# Visualización de la imagen original, la imagen con ruido y la imagen denoised\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "plt.title(\"Imagen original\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(noisy_image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "plt.title(\"Imagen con ruido añadido\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(denoised_image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "plt.title(\"Imagen denoised\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iagenerativa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

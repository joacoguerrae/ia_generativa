{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctico: Introducción a Modelos de Lenguaje usando Hugging Face y GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, instala la librería transformers si no está instalada\n",
    "# Descomenta la línea de abajo si necesitas instalar la librería\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando las librerías necesarias\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import logging as transformers_logging\n",
    "\n",
    "# Configurar el logging para que no muestre warnings de transformers\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción a los Modelos de Lenguaje\n",
    "------------------------------------------\n",
    "Un modelo de lenguaje predice la siguiente palabra en una secuencia de texto basándose en las palabras previas.\n",
    "\n",
    "En este práctico, utilizaremos un modelo preentrenado GPT-2, específicamente una versión distilada, que es más pequeña y rápida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. La Importancia de los Tokenizers\n",
    "-----------------------------------\n",
    "Los tokenizers convierten el texto en tokens numéricos, que el modelo puede entender.\n",
    "Utilizaremos el tokenizer de GPT-2 para codificar texto en tokens y decodificar los tokens de vuelta a texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargando el tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Tokenizando una oración\n",
    "sentence = \"A long time ago in a galaxy far, far away there was an astrophysicist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar la oración\n",
    "tokens = tokenizer.encode(sentence)\n",
    "print(\"Oración tokenizada:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tokens son los números que representan las palabras o partes de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodificando los tokens de nuevo a texto\n",
    "decoded_sentence = tokenizer.decode(tokens)\n",
    "print(\"Oración decodificada:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizador de GPT-2 vs Tokenizador BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el tokenizer de BERT\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"BERT tokenizer:\")\n",
    "\n",
    "# Tokenizar con BERT\n",
    "bert_tokens = ...\n",
    "print(\"- Tokens:\", bert_tokens)\n",
    "print(\"- Cantidad de tokens:\", len(bert_tokens))\n",
    "\n",
    "# Tokens individuales\n",
    "indivitual_tokens = bert_tokenizer.convert_ids_to_tokens(bert_tokens)\n",
    "print(\"- Tokens individuales: \", indivitual_tokens)\n",
    "\n",
    "# Decodificar los tokens de BERT\n",
    "bert_decoded_sentence = ...\n",
    "print(\"- Oración decodificada:\", bert_decoded_sentence, '\\n')\n",
    "\n",
    "\n",
    "print(\"GPT-2 tokenizer:\")\n",
    "# Comparar con el tokenizer de GPT-2\n",
    "gpt2_tokens = ...\n",
    "print(\"- Tokens:\", gpt2_tokens)\n",
    "print(\"- Cantidad de tokens:\", len(gpt2_tokens))\n",
    "\n",
    "# Tokens individuales\n",
    "gpt2_indivitual_tokens = ...\n",
    "print(\"- Tokens individuales: \", gpt2_indivitual_tokens)\n",
    "\n",
    "# Decodificar los tokens de GPT-2\n",
    "gpt2_decoded_sentence = ...\n",
    "print(\"- Oración decodificada:\", gpt2_decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio:\n",
    "- ¿Qué son los tokens `[CLS]` y `[SEP]` que aparecen en la *Oracion decodificada* de BERT?\n",
    "- ¿Por qué hay tokens que comienzan con `##` cuando usamos el tokenizador de BERT? ¿Qué significan? ¿Y los que no lo tienen?\n",
    "- ¿Por qué hay tokens que comienzan con `Ġ` cuando usamos el tokenizador de distill GPT-2? ¿Qué significan? ¿Y los que no lo tienen?\n",
    "- ¿Cree que son necesarios estos tokens? ¿Por qué?\n",
    "- ¿Qué pasaría si usamos el tokenizador de BERT para generar texto usando distil GPT-2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cargando el Modelo Preentrenado y el Tokenizer\n",
    "-------------------------------------------------\n",
    "Hugging Face proporciona una gran variedad de modelos preentrenados. Usaremos el modelo 'distilgpt2' para este ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poner el modelo en modo de evaluación\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Salida Cruda (Logits)\n",
    "--------------------------------------------\n",
    "Obtendremos la salida cruda del modelo (logits).\n",
    "\n",
    "Los logits son la salida del modelo antes de la capa de activación para cada token en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de prompt\n",
    "prompt = \"Yesterday, I dreamed of a world where\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar la entrada\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasar la entrada por el modelo para obtener los logits (puntajes sin normalizar)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # Salida cruda del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = logits[:, -1, :]  # Logits para el último token en el prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio:**\n",
    "\n",
    "- Explique las dimensiones de los logits.\n",
    "- ¿Qué función de activación usaría?\n",
    "- Calcule la salida del modelo del siguiente token con la función de activación que respodió en la pregunta anterior.\n",
    "- ¿Qué token elegiría y por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Selección Manual de Tokens: Top-K y Top-P Sampling\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling con Top-K:\n",
    "En Top-K sampling, seleccionamos los K tokens más probables y descartamos el resto.\n",
    "\n",
    "Esto asegura que el modelo considere solo un número limitado de tokens de alta probabilidad, lo que ayuda a evitar tokens de baja probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para aplicar Top-K sampling\n",
    "def top_k_sampling(logits, k=50):\n",
    "    # Mantener solo los K tokens con la mayor probabilidad\n",
    "    # Samplear con los tokens restantes\n",
    "    # Devolver el token sampleado\n",
    "    next_token = ...\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling con Top-P (Nucleus Sampling):\n",
    "En Top-P sampling, elegimos el conjunto más pequeño de tokens cuya probabilidad acumulada supera un umbral P.\n",
    "\n",
    "Esto significa que el modelo considera solo la porción más probable de la distribución de probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para aplicar Top-P (Nucleus) sampling\n",
    "def top_p_sampling(logits, p=0.9):\n",
    "    # Calcular la probabilidad acumulada de los tokens ordenados\n",
    "    # Remover tokens cuya probabilidad acumulada sobrepasa el umbral p\n",
    "    # Samplear usando los tokens restantes\n",
    "    # Mapear el índice de vuelta al espacio original\n",
    "    next_token = ...\n",
    "    return next_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Selección Manual de Tokens y Generación de Texto\n",
    "---------------------------------------------------\n",
    "Ahora usaremos los métodos de Top-K y Top-P para seleccionar manualmente el siguiente token y un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generación manual basada en logits\n",
    "def manual_text_generation(model, tokenizer, prompt, max_length=50, sampling_strategy=top_k_sampling, top_k=50, top_p=0.9):\n",
    "    \"\"\"\n",
    "    1) Codifique el prompt inicial\n",
    "    2) Configure un loop para generar tokens hasta alcanzar max_length o encontrar un token de fin de secuencia\n",
    "    3) En cada iteración del loop:\n",
    "        3.1) Obtenga los logits del modelo para el input\n",
    "        3.2) Samplear el siguiente token usando la estrategia de muestreo elegida (Top-K o Top-P)\n",
    "        3.3) Decodificar los tokens generado\n",
    "        3.4) Añadir el token predicho a la secuencia de entrada y continuar generando texto\n",
    "    4) Devuelva el texto generado decodificado\n",
    "    \"\"\"\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio:\n",
    "- Complete el código de la función `mual_text_generation` y genere texto jugando con los hyper-parámetros.\n",
    "- Compare salidas con las estrategias de sampleo `top p` y `top k`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experimentando con Hiperparámetros\n",
    "-------------------------------------\n",
    "Hugging face ya nos provee con una implementación de todos estos métodos. Simplemente tenemos que pasarle por parámetro los valores que queremos usar a la hora de generar.\n",
    "\n",
    "Ahora, exploremos cómo podemos controlar la salida del modelo cambiando los parámetros de generación.\n",
    "\n",
    "Intenta cambiar los valores de `max_length`, `temperature`, `top_k` y `top_p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar la entrada\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimentar con diferentes configuraciones\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=20,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodificar y mostrar el texto generado\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"\\nTexto generado con parámetros modificados:\\n\", generated_text.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio:\n",
    "- Compare este resultado con el generado en el punto anterior.\n",
    "- ¿Cómo afecta la `temperatura` a los resultados del modelo?\n",
    "- ¿Cómo afecta el `top_p` a los resultados del modelo?\n",
    "- ¿Cómo afecta la `top_k` a los resultados del modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sesgo en modelos de lenguaje\n",
    "-----------------------------------\n",
    "Ahora, exploraremos los sesgos que pueden tener estos modelos de lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio: Detectar sesgos en la generación de texto usando diferentes prompts\n",
    "prompts = [...]\n",
    "\n",
    "for _ in range(5):\n",
    "    for prompt in prompts:\n",
    "        tokens = tokenizer(prompt, return_tensors='pt')\n",
    "        output = model.generate(\n",
    "            **tokens,\n",
    "            max_length=20,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=.9,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        print(\"Prompt: \",prompt)\n",
    "        print(\"Texto generado: \", generated_text.replace('\\n', ' '))\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio:\n",
    "- ¿Nota algún sesgo en las respuestas de este modelo?\n",
    "- Proponga prompts que puedan detectar sesgos en este modelo.\n",
    "- ¿A qué se deben estos sesgos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparando con otro LM\n",
    "-----------------------------------\n",
    "Ahora, comprararemos a distilled gpt-2 contra gpt-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo distilado y completo para comparar\n",
    "distil_gpt2_model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2')  # GPT-2 completo\n",
    "\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_gpt2_model.eval()\n",
    "gpt2_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puedes ejecutar el mismo texto para ambos modelos\n",
    "prompt = \"Photosyntheis is a process that\"\n",
    "max_length = 20\n",
    "\n",
    "# Tokenizamos el texto\n",
    "\n",
    "\n",
    "print(\"Texto de entrada:\", prompt)\n",
    "\n",
    "# Generación con el modelo distilado\n",
    "print(\"\\nGeneración con el modelo distilado:\")\n",
    "\n",
    "inference_time_distil = ...\n",
    "generated_text_distil = ...\n",
    "\n",
    "print(\"- Texto generado: '\", generated_text_distil, \"'\")\n",
    "print(\"- Tiempo de inferencia: \", inference_time_distil)\n",
    "\n",
    "\n",
    "# Generación con el modelo completo\n",
    "print(\"\\nGeneración con el modelo completo:\")\n",
    "\n",
    "inference_time_gpt2 = ...\n",
    "generated_text_gpt2 = ...\n",
    "\n",
    "print(\"- Texto generado por GPT-2: '\", generated_text_gpt2, \"'\")\n",
    "print(\"- Tiempo de inferencia: \", inference_time_gpt2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio:\n",
    "\n",
    "- Compare los dos modelos y sus resultados usando diferentes *prompts* y valores de *max_length*.\n",
    "- ¿Es necesario usar el parámetro `max_legth`? ¿Por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Entendiendo la Salida del Modelo\n",
    "-----------------------------------\n",
    "Después de generar algunos ejemplos, reflexiona sobre la calidad del texto generado:\n",
    " - ¿Considera que estos modelos generan texto coherente?\n",
    " - ¿Qué tan creativo o repetitivo es el texto?\n",
    " - ¿Hay ejemplos donde la generación falla o produce resultados sin sentido?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IAGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
